---
layout: post
title:  How to fetch Twitter users with R
date: "2017-05-10 08:11:29 UYT"
published: true
tags: [rstats, r, Twitter, rtweet, purrr, map, ggmap]
description: How to fetch Twitter users and clean the data using R!
---
Here I show how to fetch Twitter users using the `rtweet` package, and clean the data using the `tidyverse` set of packages, for later usage in plotting animated maps.  

<!--more-->

I recently [came across this article](http://spatial.ly/2017/03/mapping-5000-years-of-city-growth/), and I knew I had to make a similar map for the [R-Ladies' chapters](http://rladies.org/) (probably the purple color had plenty to do with that `r emo::ji("purple_heart")` ). So my idea was to map all the R-Ladies' chapters according to their size, and that's when I thought of using their Twitter followers as a way to estimate it, since it's the most extended social media we use (except for some chapters).  

If I wanted to show everything I've done in a single post, it would be almost as long as my first one! And I didn't want that `r emo::ji("stuck_out_tongue_closed_eyes")` . So I decided to make 3 _tutorial-like_ posts: 
- one for the data preparation (this one)
- one about [plotting an interactive map using `plotly` and how to avoid huge HTML content]({% post_url 2017-04-26-how-to-deal-with-ggplotly-huge-maps %}){:target="_blank"}
- the last one about [making the maps and animating them using `gganimate`]({% post_url 2017-04-24-how-to-plot-animated-maps-with-gganimate %}){:target="_blank"} 
 
Finally [there is other post where I just go through what I did without getting into too many details]({% post_url 2017-05-10-visualizing-r-ladies-growth %}){:target="_blank"}, focusing on the visualisation results. 

So here I go!

## Getting Twitter users

I had to learn how to retrieve data from the Twitter API, and I chose to use the `rtweet` package, which is super easy to use! Since I only use public data I don't have to worry about getting my Twitter personal access token. 

Every R-Ladies' chapter uses a standard handle, with the *RLadiesLocation* format (thankfully they are very compliant with this!). By setting the `q` parameter to `'RLadies'` I'm setting the query to be searched. `n = 1000` sets the number of users to retrieve, being 1000 the maximum from a single search. As I want a dataframe as a result, I set the `parse` parameter to `TRUE`.

```{r load_data, echo = FALSE, message = FALSE, warning = FALSE}
# The data isn't in this repository, you can find everything I use here:
# https://github.com/d4tagirl/R-Ladies-growth-maps

library(knitr)
knitr::opts_chunk$set(dpi = 130, fig.align = 'center', screenshot.force = FALSE, fig.cap = "")
options(width = 80, dplyr.width = 150)

library(readr)
library(dplyr)

url_csv <- 'https://raw.githubusercontent.com/d4tagirl/R-Ladies-growth-maps/master/users.csv'
users <- read_csv(url(url_csv)) %>% 
  select(-1)

```

```{r  false_load_data, eval = FALSE}
library(rtweet)

users <- search_users(q = 'RLadies',
                      n = 1000,
                      parse = TRUE)
```

Let's see what it returns:

```{r message = FALSE, warning = FALSE}
library(DT)
datatable(users[, c(2:5)], rownames = FALSE,
          options = list(pageLength = 5))
```

<br/>
It retrieves the user if it matches the user's _description_ as well as _name_ and _screen\_name_ (handle), with 36 variables regarding the user. I'm only showing the ones I'm going to use, but there is a lot of extra information there.

I used `DT::datatable` just in case someone wants to go through whats on the whole table (of course here I'm thinking about the R-Ladies community here `r emo::ji("heart_eyes")` ). It was not easy to set up the environment for my blog to show this table (it uses `HTML widgets`), but luckily my hubby was more than willing to help me with that part `r emo::ji("sweat_smile")` . If you are using RStudio it is just as simple as installing the `DT` package, or you can always use `knitr::kable(head(users[, c(2:5)]), format = "html")` to see the first rows.

## Cleaning the data

First I make sure I don't have any duplicates, and then I keep only the handles that comply with the stipulated format, using a regular expression. I filter out 3 additional handles: 
- _'RLadies'_, whose _name_ is _'Royal Ladies'_ and I assume has something to do with royalty by the crown on their profile picture `r emo::ji("princess")` 
- _'RLadies\_LF'_, a Japanese account that translated as follows on _Google Translator_: _'Rakuten Ichiba fashion delivery'_
- _'RLadiesGlobal'_, because it is not a chapter, so I don't want to include it on the plot. 

Then I format the _date class_ variable `created_at` as `%Y-%m-%d` (just because seeing the hours, minutes and seconds annoys me!), generate the age in days `age_days` (for reproducibility, I set a fixed date to calculate it) and select the variables I will use for my analysis.

```{r  head_rladies, message = FALSE, warning = FALSE}
library(dplyr)
library(lubridate)
library(stringr)
library(tidyr)

rladies <- unique(users) %>%
  filter(str_detect(screen_name, '^(RLadies).*') & 
           !screen_name %in% c('RLadies', 'RLadies_LF', 'RLadiesGlobal')) %>% 
  mutate(created_at = format(as.Date(created_at), format = '%Y-%m-%d'),
         age_days = difftime(as.Date('2017-5-15'), created_at, unit = 'days')) %>%
  select(screen_name, location, created_at, followers = followers_count, age_days)
```

One final fix: I have some missing values on `location` that I'll need for geocoding the chapters, so I use an auxiliary table `lookup` to match the `screen_name` with the `location`, using `dplyr::left_join`. 

```{r , message = FALSE, warning = FALSE}
library(tibble)
lookup <- tibble(screen_name = c('RLadiesLx', 'RLadiesMTL' , 'RLadiesSeattle'), 
                 location    = c('Lisbon'   , 'Montreal'   , 'Seattle'      ))

rladies <- rladies %>%
  left_join(lookup, by = 'screen_name') %>%
  mutate(location = coalesce(location.x, location.y)) %>%
  select(-location.x, -location.y)
```

There are two additional chapters with no presence on Twitter: one in _Taipei, Taiwan_, and the other in _Warsaw, Poland_. I add them according to their creation date, using the number of members on their Meetup account as followers.

```{r  message = FALSE, warning = FALSE}
rladies <- rladies %>% 
  add_row(      
    screen_name = 'RLadiesTaipei',
    location = 'Taipei',
    created_at = as.Date('2014-11-15'),
    followers = 347) %>% 
  add_row(      
    screen_name = 'RLadiesWarsaw',
    location = 'Warsaw',
    created_at = as.Date('2016-11-15'),
    followers = 80)

datatable(rladies, rownames = FALSE,
          options = list(pageLength = 5))
```

<br />
As my ultimate goal is to plot the chapters on a map, I need to obtain the _latitude_ and _longitude_ for each one of them. That's when the `ggmap` package really comes in handy: it interacts with _Google Maps_ to retrieve latitude and longitude, and I don't even have to worry about getting the location into a specific format, because it is so good that it doesn't need it! (my first try was actually by extracting the cities, but many of the chapters didn't match or matched wrongly, so I tried it this way and it worked perfectly!)

Since the `ggmap::geocode` function returns 2 columns, I thought about calling it twice: once for the longitude and once for the latitude. But I didn't like it because it was awfully inefficient, and the geocoding takes some (really long!) time. It was going to be something like this:

```{r  false2, eval = FALSE}
library(ggmap)

rladies <- rladies %>% 
  mutate(lon = geocode(location)[,1],
         lat = geocode(location)[,2])
```

Doing some research I finally decided to use the `purrr::map` function for capturing both values in a single column of the dataframe, and then transform it into two separate columns with `tidyr::unnest`. All of this with never having to leave the `tidyverse` world `r emo::ji("smirk")`

I'm doing it in two steps to see the intermediate result, with the two columns in a single variable of the dataframe.

```{r false3, eval = FALSE}
library(ggmap)
library(purrr)

rladies <- rladies %>% 
  mutate(longlat = purrr::map(.$location, geocode)) 
```

```{r echo = FALSE, message = FALSE, warning = FALSE, screenshot.force = FALSE}
rladies <- readRDS(gzcon(url('https://github.com/d4tagirl/R-Ladies-growth-maps/raw/master/rladies_longlat.rds')))

kable(head(rladies), format = "html")
```

```{r}
rladies <- rladies %>% 
  unnest() 
```

```{r echo = FALSE}
datatable(rladies, rownames = FALSE,
          options = list(pageLength = 5))        
```

<br />
That's it!

Now the dataframe is ready for me to use it for visualizing these Twitter users on a map (considering their sizes and dates of creation), and make some interactive maps and animations! If you are interested, you can check how I do it [in this post]({% post_url 2017-05-10-visualizing-r-ladies-growth %}){:target="_blank"}. You can also go to my [GitHub repo to see the code](https://github.com/d4tagirl/R-Ladies-growth-maps).

Please leave your comments and suggestions below or [mention me on Twitter](https://twitter.com/intent/tweet?user_id=114258616). Thanks for reading `r emo::ji("wink")`
